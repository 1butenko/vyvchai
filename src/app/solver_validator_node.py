import os
import re
from typing import List, Literal, TypedDict

from langchain_core.messages import HumanMessage
from .lapa_config import get_lapa_llm

# --- Pre-load the LLM for solving ---
# It can be the same model, but used with a different, specialized prompt.
SOLVER_LLM = get_lapa_llm(temperature=0.0)

# --- Define State & Data Structures ---


class GeneratedTask(TypedDict):
    """Represents a single task generated by a previous node."""

    task_text: str
    answer_key: str


class ValidationResult(TypedDict):
    """The output of this node, defining the status and providing feedback."""

    validation_status: Literal["VALIDATED", "REGENERATE"]
    validated_tasks: List[GeneratedTask]
    feedback_for_regeneration: List[dict]


# --- The Solver/Validator Node ---

SOLVER_PROMPT_TEMPLATE = """
Ти — експерт з математики, який перевіряє завдання для 8-9 класів.
Твоя мета — розв'язати надане завдання і дати ОДНОЗНАЧНУ, КОРОТКУ відповідь.

Завдання:
"{task_text}"

Розв'яжи його. Поверни ТІЛЬКИ фінальну відповідь, обгорнуту в теги <answer>.
Наприклад:
Завдання: "2x + 5 = 13"
Відповідь: "<answer>4</answer>"

Завдання: "Знайди корінь рівняння x^2 - 9 = 0"
Відповідь: "<answer>3, -3</answer>"
"""


def solver_validator_node(state: dict) -> dict:
    """
    Evaluates generated tasks by attempting to solve them.
    Implements the Evaluator-Optimizer pattern.
    """
    print("---NODE: SOLVER/VALIDATOR---")
    if SOLVER_LLM is None:
        print("ERROR: Solver LLM not initialized.")
        # If solver can't run, we have to assume tasks are valid to not break the chain
        return {
            "validation_status": "VALIDATED",
            "validated_tasks": state.get("generated_tasks", []),
            "feedback_for_regeneration": [],
        }

    generated_tasks: List[GeneratedTask] = state.get("generated_tasks", [])
    discipline_id = state.get("global_discipline_id")

    if not generated_tasks:
        print("No tasks to validate.")
        return {
            "validation_status": "VALIDATED",
            "validated_tasks": [],
            "feedback_for_regeneration": [],
        }

    validated_tasks = []
    feedback_for_regeneration = []

    for i, task in enumerate(generated_tasks):
        print(f"Validating task {i+1}/{len(generated_tasks)}...")

        # For now, we apply strict validation only to Algebra, as requested
        if discipline_id != 72:
            validated_tasks.append(task)
            continue

        # 1. Agent tries to solve the task
        prompt = SOLVER_PROMPT_TEMPLATE.format(task_text=task["task_text"])
        response = SOLVER_LLM.invoke([HumanMessage(content=prompt)])

        # 2. Extract the answer from <answer> tags for robust parsing
        match = re.search(r"<answer>(.*?)</answer>", response.content, re.DOTALL)

        if not match:
            feedback = {
                "original_task": task,
                "reason": "Solver failed to provide an answer in the correct format <answer>...</answer>.",
            }
            feedback_for_regeneration.append(feedback)
            continue

        solved_answer = match.group(1).strip().lower()
        expected_answer = task["answer_key"].strip().lower()

        # 3. Validation Logic
        if solved_answer == expected_answer:
            print(
                f"Task {i+1} VALIDATED. Solver answer '{solved_answer}' matches key '{expected_answer}'."
            )
            validated_tasks.append(task)
        else:
            print(
                f"Task {i+1} FAILED. Solver answer '{solved_answer}' does not match key '{expected_answer}'."
            )
            feedback = {
                "original_task": task,
                "reason": f"Validation failed. Expected answer '{expected_answer}', but the solver's answer was '{solved_answer}'. The task may be ambiguous or the key incorrect.",
            }
            feedback_for_regeneration.append(feedback)

    # 4. Determine final status
    if feedback_for_regeneration:
        print("Found invalid tasks. Requesting regeneration.")
        return {
            "validation_status": "REGENERATE",
            "validated_tasks": validated_tasks,
            "feedback_for_regeneration": feedback_for_regeneration,
        }
    else:
        print("All tasks validated successfully.")
        return {
            "validation_status": "VALIDATED",
            "validated_tasks": validated_tasks,
            "feedback_for_regeneration": [],
        }


# --- Example Usage ---
if __name__ == "__main__":
    if os.environ.get("GOOGLE_API_KEY"):
        print("\n--- Testing Solver/Validator Node ---")

        # Simulate a list of generated tasks from a previous node
        dummy_tasks = [
            {"task_text": "Знайди x, якщо 2x + 3 = 11.", "answer_key": "4"},  # Correct
            {
                "task_text": "Знайди x, якщо x^2 - 16 = 0.",
                "answer_key": "4",
            },  # Incorrect key, solver should get "4, -4"
            {
                "task_text": "Яка столиця України?",
                "answer_key": "Київ",
            },  # Non-algebra task
            {"task_text": "Спрости вираз: 2(a+b) - 2a", "answer_key": "2b"},  # Correct
        ]

        # Simulate state for Algebra (id=72)
        algebra_state = {"generated_tasks": dummy_tasks, "global_discipline_id": 72}

        print("\n--- SCENARIO 1: ALGEBRA (id=72) ---")
        result = solver_validator_node(algebra_state)
        import json

        print(json.dumps(result, indent=2, ensure_ascii=False))

        # Simulate state for another discipline (e.g., History, id=107)
        history_state = {"generated_tasks": dummy_tasks, "global_discipline_id": 107}

        print("\n--- SCENARIO 2: HISTORY (id=107) ---")
        result_history = solver_validator_node(history_state)
        print(json.dumps(result_history, indent=2, ensure_ascii=False))

    else:
        print("\nSkipping solver test: GOOGLE_API_KEY is not set.")
